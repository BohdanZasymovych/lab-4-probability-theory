---
title: "Lab 4"
output: html_notebook
---

Work breakdown:

-   Maxym Prokopetz: Task 1

-   Bohdan Zasymovych: Task 3

-   Andrii Kryvyi: Task 2 & 4

```{r}
library(ggplot2)
```

## Task 1

### Hypotheses

We have two samples:

-   $X_1,\dots,X_{100} \sim \mathcal{N}(\mu_1, 1)$
-   $Y_1,\dots,Y_{50} \sim \mathcal{N}(\mu_2, 1)$

We test: $$
H_0: \mu_1 = \mu_2
\qquad\text{vs}\qquad
H_1: \mu_1 \ne \mu_2
$$

Since the variances are known and fixed at $1$, the appropriate procedure is the two-sample z-test for the difference of means.

### Test statistic

$$
Z = \frac{\bar{X} - \bar{Y}}{\sqrt{\frac{1}{n_1} + \frac{1}{n_2}}}
$$

Under the null hypothesis, $$
Z \sim \mathcal{N}(0, 1).
$$

### Critical region (α = 0.05)

$$
C_{0.05} = \bigl\{\, |Z| > 1.96 \,\bigr\}.
$$

### R code

```{r}
id <- 22
n <- id
set.seed(id)

a_k <- 1:150


func <- function(k) {
    full_value <- k * log(k^2 * n + pi)
    fractional_part <- full_value - floor(full_value)

    return(fractional_part)
}


x <- sapply(a_k[1:100], func)
x <- qnorm(x)
y <- sapply(a_k[101:150], func)
y <- qnorm(y)

x_mean <- mean(x)
y_mean <- mean(y)

n1 <- length(x)
n2 <- length(y)

z_value <- (x_mean - y_mean) / sqrt(1/n1 + 1/n2)
p_value <- 2 * (1 - pnorm(abs(z_value)))


cat(z_value, p_value)
```

### Conclusion

The p-value (equal 0.53...) is greater than 0.05, and \|Z\| (equal 0.62...) is below 1.96. Therefore, we do not reject null hypothesis. The data do not provide statistical evidence that the means differ.

# Task 2

## Theoretical derivation

### Lemma 1

Before the actual derivation we will need some equalities:

$$
\sum_{k=1}^n(X_k-\mu_x)^2 = \sum_{k=1}^nX_k^2 - 2\mu_x\sum_{k=1}^n X_k + n\mu_x^2\\
= \Big[\sum_{k=1}^nX_k^2 - 2n\overline{\mathbf{X}}^2+ n\overline{\mathbf{X}}^2\Big]+ \Big[n\overline{\mathbf{X}}^2 - 2n\mu_x\overline{\mathbf{X}} + n\mu_x^2\Big] \\
= \sum_{k=1}^n(X_k - \overline{\mathbf{X}})^2 + n(\overline{\mathbf{X}}-\mu_x)^2
$$

From the above follows that:

$$
\sum_{k=1}^n(\frac{X_k-\mu_x}{\sigma_x})^2 = \sum_{k=1}^n(\frac{X_k - \overline{\mathbf{X}}}{\sigma_x})^2 + (\sqrt{n}\frac{\overline{\mathbf{X}}-\mu_x}{\sigma_x})^2
$$

Note that $\frac{X_k-\mu_x}{\sigma_x} \sim \mathcal{N}(0, 1)$, because we assumed that $X_k \sim \mathcal{N}(\mu_x, \sigma_x^2)$. Thus and because of independence we have following:

$$
\sum_{k=1}^n(\frac{X_k-\mu_x}{\sigma_x})^2  \sim \chi^2_n
$$

And also because $\overline{\mathbf{X}} \sim \mathcal{N}(\mu_x, \frac{\sigma_x^2}n)$:

$$
(\sqrt{n}\frac{\overline{\mathbf{X}}-\mu_x}{\sigma_x})^2 \sim \chi^2_1
$$

To make r.h.s of the equality have the same distribution as the l.h.s we need:

$$
\sum_{k=1}^n(\frac{X_k - \overline{\mathbf{X}}}{\sigma_x})^2  = \frac{S_\mathbf{xx}}{\sigma_x^2}\sim \chi^2_{n-1}
$$

In the same way we can show that $\frac{S_\mathbf{yy}}{\sigma_y^2}\sim \chi^2_{m-1}$.

### Lemma 2

Since samples $\mathbf{X}$ and $\mathbf{Y}$ are independent, then $\frac{S_\mathbf{xx}}{\sigma_x^2}$ and $\frac{S_\mathbf{yy}}{\sigma_y^2}$ are independent random variables with Chi-squared distributions.

Remember the definition of Fisher distribution with $d_1$ and $d_2$ degrees of freedom:

$$
\mathcal{F}^{d_1}_{d_2} = \frac{U_1/d_1}{U_2/d_2} \qquad U_1\sim\chi^2_{d_1} \qquad U_2\sim\chi^2_{d_2} \qquad U_1\ \text{and}\ U_2\ \text{independent}
$$

Thus from lemma 1 we conclude that

$$
\Big[\frac{S_{\mathbf{xx}}/(n-1)}{S_{\mathbf{yy}}/(m-1)} \mid \sigma_x^2=\sigma_y^2\Big] = \frac{S_{\mathbf{xx}}\sigma_y^2(m-1)}{\sigma_x^2S_{\mathbf{yy}}(n-1)} \sim \mathcal{F}^{n-1}_{m-1}
$$

### Derivation

Our hypothesis is: $H_0: \sigma_x = \sigma_y\quad \text{v.s.}\quad H_1:\sigma_x\neq\sigma_y$.

Denote f-statistic as follows:

$$
f(\mathbf{x}, \mathbf{y}) = \frac{S_{\mathbf{xx}}/(n-1)}{S_{\mathbf{yy}}/(m-1)}
$$

We have to build $C(\alpha) \subset \mathbb{R}^{n}\times\mathbb{R}^{m}$ s.t. $P((\mathbf{x}, \mathbf{y}) \in C(\alpha)\mid H_0) \leq \alpha$.

We can use the lemma 2 here:

$$
P(f(\mathbf{x}, \mathbf{y}) \in \big[\mathcal{F}^{n-1,m-1}_{\alpha/2}, \mathcal{F}^{n-1,m-1}_{1-\alpha/2}\big] \mid  H_0) = 1-\alpha
$$

Equivalently:

$$
P(f(\mathbf{x}, \mathbf{y}) \notin \big[\mathcal{F}^{n-1,m-1}_{\alpha/2}, \mathcal{F}^{n-1,m-1}_{1-\alpha/2}\big] \mid  H_0) = \alpha
$$

So critical region is:

$$
C(\alpha) = \{(\mathbf{x}, \mathbf{y}) \in \mathbb{R}^n\times\mathbb{R}^m \mid f(\mathbf{x}, \mathbf{y}) \notin \big[\mathcal{F}^{n-1,m-1}_{\alpha/2}, \mathcal{F}^{n-1,m-1}_{1-\alpha/2}\big]\}
$$

Minimizing the $\alpha$ given sample $(\mathbf{x}, \mathbf{y})$ we will get:

$$
p = 2\min\{F_{\mathcal{F}^{n-1}_{m-1}}(f(\mathbf{x}, \mathbf{y})), 1-F_{\mathcal{F}^{n-1}_{m-1}}(f(\mathbf{x}, \mathbf{y}))\}
$$ This test is called f-test.

## Rejection region for the significance level 0.05 and p-value

As described above:

$$
C(0.05) = \{(\mathbf{x}, \mathbf{y}) \in \mathbb{R}^n\times\mathbb{R}^m \mid f(\mathbf{x}, \mathbf{y}) \notin \big[\mathcal{F}^{n-1,m-1}_{0.025}, \mathcal{F}^{n-1,m-1}_{0.975}\big]\}
$$

```{r}
f <- (sum((x - mean(x))^2)/(length(x)-1))/(sum((y - mean(y))^2)/(length(y)-1))
print(sprintf("Statistic f(x, y): %.6f", f))
q1 <- qf(0.025, length(x)-1, length(y)-1)
q2 <- qf(0.975, length(x)-1, length(y)-1)
print(sprintf("Complement of rejection region for statistic: [%.6f, %.6f]", q1, q2))

if(f < q1 || f > q2) {
  print("We reject the H0")
} else {
  print("We do not reject the H0")
}

p <- 2*min(pf(f, length(x)-1, length(y)-1), 1-pf(f, length(x)-1, length(y)-1))
print(sprintf("p-value: %.6f", p))
```

```{r}
var.test(x, y, alternative="two.sided")
```

## Conclusion

So we can see that for the significance level of $0.05$ we **do not** reject $H_0$. Our p-value is pretty high - so we do not reject the $H_0$ for even larger values of $\alpha$. Thus we conclude that there is a possibility for variances of $Y$ and $X$ to be equal.

# Task 3

## Idea behind KS test

Kolmogorov-Smirnov (KS) test is a test which is used to check whether the probability distributions of a sample and a control distribution, or two samples are equal. It is constructed based on the cumulative distribution function (CDF) and calculates the greatest difference between the empirical cumulative distribution function (ECDF) of the sample and the theoretical or empirical distribution of the control sample.

The Kolmogorov-Smirnov test is mostly used for two purposes:\
- One-sample KS test: To compare the sample distribution to a known reference distribution.\
- Two-sample KS test: To compare the two independent samples' distributions.

**Hypotheses:**

$$
H_0: F_X = F\newline
H_1: F_X \ne F
$$

**Test statistic:**

$$
D = \sup_{t \in R}|F_X(t) - F(t)|
$$

Where:\
- $F_X(t)$ - ECDF of given sample\
- $F(t)$ - CDF of control distribution for one-sample test or ECDF of the second sample for two-sample test

Test statistic follows Kolmogorov distribution

By LLN ECDF becomes close to true CDF so under $H_0$ realizations of test statistic should take small values.

**Critical region of level** $\alpha$:\
$$
C_\alpha = \left\{ \textbf{x} \in R^n | d \ge d_{1-\alpha} \right\}
$$

Where:\
- $d$ - realization of r. v. $D$\
- $d_{1-\alpha}$ - quantile of Kolmogorov distribution of level $1-\alpha$

**p-value:**

$$
p(\textbf{x}) = 1 - F_D(d)
$$

Where $F_D$ is CDF of Kolmogorov distribution

## Tests

### (a)

Test whether $\{x_k\}_{k=1}^{100}$ are normally distributed (with parameters calculated from the sample)

$$
H_0: F = F_{\mathcal{N} ( \bar{\textbf{x}}, s^2)} \newline
H_1: F \ne F_{\mathcal{N} ( \bar{\textbf{x}}, s^2)}
$$

```{r}
sample_mean <- mean(x)
sample_sd <- sd(x)
ks_norm_result <- ks.test(x, "pnorm", mean=sample_mean, sd=sample_sd)

print(ks_norm_result)
```

```{r}
data_x <- data.frame(Value = x)

ecdf_vs_cdf_plot <- ggplot(data_x, aes(x = Value)) +
  
  stat_ecdf(
    aes(color = "Sample ECDF"), 
    geom = "step", 
    linewidth = 1.2
  ) +

  stat_function(
    aes(color = "Theoretical CDF"), 
    fun = pnorm, 
    args = list(mean=sample_mean, sd=sample_sd),
    linewidth = 1
  ) +
  
  labs(
    title = paste("Sample ECDF vs. Normal(", round(sample_mean, 4), ",", round(sample_sd**2, 4), ") CDF"),
    x = "Observed Value (x)",
    y = "CDF (F(x))",
    color = "Function"
  ) +
  scale_color_manual(values = c("Sample ECDF" = "blue", "Theoretical CDF" = "red")) +
  theme_minimal()

ecdf_vs_cdf_plot
```

**Conclusion:**\
p-value of a test is $\approx 0.61$, it is greater than desired significance level $\alpha = 0.05$, so $H_0$ is not rejected.

### (b)

Test whether $\{|x_k|\}_{k=1}^{100}$ are exponentially distributed with $\lambda = 1$;

$$
H_0: F = F_{\mathcal{E}(1)} \newline
H_1: F \ne F_{\mathcal{E}(1)}
$$

```{r}
lambda <- 1
x_abs <- abs(x)
ks_exp_result <- ks.test(x_abs, "pexp", rate=lambda)

print(ks_exp_result)
```

```{r}
data_x <- data.frame(Value = x_abs)

ecdf_vs_cdf_plot <- ggplot(data_x, aes(x = Value)) +
  
  stat_ecdf(
    aes(color = "Sample ECDF"), 
    geom = "step", 
    linewidth = 1.2
  ) +

  stat_function(
    aes(color = "Theoretical CDF"), 
    fun = pexp, 
    args = list(rate=lambda),
    linewidth = 1
  ) +
  
  labs(
    title = paste("Sample ECDF vs. Exp(", lambda, ") CDF"),
    x = "Observed Value (x)",
    y = "CDF (F(x))",
    color = "Function"
  ) +
  scale_color_manual(values = c("Sample ECDF" = "blue", "Theoretical CDF" = "red")) +
  theme_minimal()

ecdf_vs_cdf_plot
```

**Conclusion:**\
p-value of a test is $\approx 0.07$, it is greater than desired significance level $\alpha = 0.05$, so $H_0$ is not rejected.

### (c)

Test whether $\{x_k\}_{k=1}^{100}$ and $\{y_l\}_{l=1}^{50}$ have the same distributions.

$$
H_0: F_X = F_Y \newline
H_1: F_X \neq F_Y
$$

```{r}
ks_two_sample_result <- ks.test(x, y)

print(ks_two_sample_result)
```

```{r}
data_combined <- data.frame(
  Value = c(x, y),
  Group = factor(c(rep("x", length(x)),
                   rep("y", length(y))))
)

ecdf_comparison_plot <- ggplot(data_combined, aes(x = Value, color = Group)) +
  
  stat_ecdf(
    geom = "step",
    linewidth = 1.2
  ) +
  
  labs(
    title = "sample ECDF (x) vs. sample ECDF (y)",
    x = "Observed Value",
    y = "ECDF (F(x))",
    color = "Data Sample"
  ) +

  scale_color_manual(values = c("x" = "blue", 
                               "y" = "red")) +
  theme_minimal()


ecdf_comparison_plot
```

**Conclusion:**\
p-value of a test is $\approx 0.43$, it is greater than desired significance level $\alpha = 0.05$, so $H_0$ is not rejected.

## Results

3 hypothesis were tested for all of them $H_0$ was not rejected

Samples were formed such that values drawn from the uniform distribution with parameters 0 and 1 were input into the inverse CDF of the standard normal distribution ($\Phi^{-1}$); thus, the samples had a distribution close to standard normal. Because of this, for the test in subtask (a), the result that $H_0$ is not rejected was expected. The same applies to the test in subtask (c): both samples have approximately the same distribution, close to standard normal.

In subtask (b), $H_0$ is not true but was not rejected, so a Type II error occurred. This happened because the sample size was not large enough and the approximation was not accurate enough. When we increased the sample sizes, the $H_0$ hypothesis in subtasks (a) and (c) was not rejected, and the p-values were significantly greater than the desired significance level $\alpha = 0.05$. In subtask (b), for larger samples, the p-value became much less than $0.05$, and $H_0$ was rejected. Thus, increasing the sample size increases the accuracy of the test.

In subtask (a) distribution of the sample was tested against the normal distribution with parameters obtained from the same sample. KS test is designed to test such that distribution equality to which is tested should not be dependent on sample, so in this case small bias towards equality of distributions may occur.

# Task 4

## Reading dataset

```{r}
dataset <- read.csv("data.csv")
dataset
```

## Scatter plot

```{r}
x <- dataset$time_study
y <- dataset$Marks

plot(x, y, xlab="Study Time", ylab="Marks", col="red", pch=19)
```

## Fitting model and results evaluation

We are assuming that the data comes from the following distribution:

$$
Y_k = a + bx_k + \varepsilon_k, \qquad \varepsilon_k \sim \mathcal{N}(0, \sigma^2)\ \text{i.i.d.}
$$

We have fixed $\mathbf{x}$ and sample $\mathbf{Y}$ and $3$ parameters: $a$, $b$ and $\sigma^2$. We can use MLE to estimate them. The MLE will be equivalent to the OLS estimator, which we can get by minimizing the function:

$$
R = \sum_{k=1}^n(Y_k-a-bx_k)^2
$$

Minimizing this function via calculus, we will get the following results:

$$
\hat{a} = \overline{\mathbf{Y}}-\hat{b}\overline{\mathbf{x}} \qquad \hat{b} = \frac{S_\mathbf{xY}}{S_\mathbf{xx}} = \frac{\sum_{k=1}^n(x_k-\overline{\mathbf{x}})(Y_k - \overline{\mathbf{Y}})}{\sum_{k=1}^n(x_k-\overline{\mathbf{x}})^2}\\
\qquad \hat{\sigma}^2 = \frac{\hat{R}}{n-2} = \frac{1}{n-2}\sum_{k=1}^n(Y_k-\hat{a}-\hat{b}x_k)^2
$$

As a result we will have a following fitted model:

$$
\hat{y}(x) = \hat{a} + \hat{b}x
$$

```{r}
model <- lm(y ~ x)

plot(x, y, xlab="Study Time", ylab="Marks", col="red", pch=19)
abline(model, col="blue", lwd=2)
```

```{r}
summary(model)
```

### Goodness-of-fit

To evaluate the goodness-of-fit we can use the determination coefficient which gives us a percentage of variance in our original data that is explained by the fitted model:

$$
r^2 = 1 - \frac{\sum_{k=1}^n(Y_k-\hat{y}(x_k))^2}{\sum_{k=1}^n(Y_k-\overline{\mathbf{Y}})^2}
$$

In our case we get that $r^2 = 0.8878$, this value is pretty high and we can conclude that the model fits well.

### Testing significance

To check whether study time is significant in predicting marks we can test whether we have a non-zero slope. If so, we will have some dependence of one variable on another. Formally our hypothesis is:

$$
H_0: b = 0 \quad \text{v.s.} \quad H_1: b \neq 0
$$

Recap the formula for $\hat{b}$:

$$
\hat{b} = \frac{\sum_{k=1}^n(x_k-\overline{\mathbf{x}})(Y_k - \overline{\mathbf{Y}})}{\sum_{k=1}^n(x_k-\overline{\mathbf{x}})^2}
$$

We can see that the estimator linearly depends on a sample $Y_k$. Since all $Y_k$ are i.i.d. r.v. with a normal distribution, $\hat{b}$ is also a r.v. with a normal distribution:

$$
\hat{b} \sim \mathcal{N}(b, \frac{\sigma^2}{S_\mathbf{xx}})
$$

Therefore we can apply a t-test for that hypothesis.

Our t-statistic will be:

$$
t = \frac{\hat{b}-0}{SE(\hat{b})} = \frac{\hat{b}}{\hat{\sigma}}\sqrt{S_\mathbf{xx}} = \frac{\hat{b}}{\sqrt{\hat{R}}}\sqrt{S_\mathbf{xx}(n-2)}
$$

The p-value is calculated as follows (since we are doing a two-sided version of the test):

$$
p = 2F_{\mathcal{T}_{n-2}}(-|t|)
$$

From the summary of the model, we can see that the p-value for the t-test on the scale parameter is smaller than the value R is possible to calculate. This means that we can reject the null-hypothesis for very small alpha, thus we conclude that $b \neq 0$ and there is a dependence of grades over time of study.

## Alice grade prediction

```{r}
alice.hours <- 8
alice.marks <- predict.lm(model, data.frame(x=alice.hours))
print(sprintf("ŷ(%.2f) = %.2f", alice.hours, alice.marks))
```

So for 8 hours of study we predict that Alice will have a grade of $46.7$.

## Possible improvements

### Improvement 1

We can collect more data about students. This way we will have larger sample from $Y$ population, therefore as a consequence of law of large numbers predicted value will converge to theoretical mean.

### Improvement 2

Notice on the plot that our data is actually non-linear. We can see that it i s convex, because in the middle our model overestimates and on the boundaries it underestimates. We can fix it by including additional basis function, making our model polynomial. Yet we can still use linear regression on $\hat{y} = a + bx + cx^2$ with $a$, $b$ and $c$ as parameters.

```{r}
model <- lm(y ~ x + I(x^2))

plot(x, y, xlab="Study Time", ylab="Marks", col="red", pch=19)
curve(predict.lm(model, newdata = data.frame(x = x)), 
      add = TRUE, col = "blue", lwd = 2)
summary(model)
```

We can see that the determination coefficient grew: instead of 89% of variance we can now explain 96% - the model fits very well.

## Conclusions

After fitting a linear model to data, checking determination coefficient and performing a t-test on the scale parameter we conclude that data has some linear nature and grade does depend on study time. Yet still there is some non-linearity, because model $y = a+bx+cx^2$ performs even better than $y = a+bx$.
